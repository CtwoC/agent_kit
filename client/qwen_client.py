"""Qwen API å®¢æˆ·ç«¯"""
import json
from dataclasses import dataclass
from openai import AsyncOpenAI
from typing import Dict, Any, AsyncIterator, List, Optional
from base_client import BaseLLMClient, Tool, Usage, ModelPrices
from utils.retry import async_retry, stream_async_retry

@dataclass
class ToolResult:
    """å·¥å…·è°ƒç”¨ç»“æœ
    
    Args:
        tool_name: å·¥å…·åç§°
        tool_result: å·¥å…·è°ƒç”¨ç»“æœ
    """
    tool_name: str
    tool_result: Any

class QwenClient(BaseLLMClient):
    """Qwen API å®¢æˆ·ç«¯
    
    ä¸“é—¨é’ˆå¯¹é€šä¹‰åƒé—® API çš„å®¢æˆ·ç«¯å®ç°ã€‚
    - éæµå¼å¯¹è¯ä¸ OpenAI client å®Œå…¨ç›¸åŒ
    - æµå¼å¯¹è¯ä½¿ç”¨æ ‡å‡†çš„ Chat Completions API (stream=True)
    """
    
    def __init__(
        self,
        api_key: str,
        model: str = "qwen-plus",
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        **kwargs
    ):
        # å°† LLM ç›¸å…³å‚æ•°ä» kwargs ä¸­åˆ†ç¦»å‡ºæ¥
        mcp_kwargs = {}
        if 'mcp_urls' in kwargs:
            mcp_kwargs['mcp_urls'] = kwargs.pop('mcp_urls')
        if 'enable_timeout_retry' in kwargs:
            mcp_kwargs['enable_timeout_retry'] = kwargs.pop('enable_timeout_retry')
            
        super().__init__(api_key, **mcp_kwargs)
        
        # åˆå§‹åŒ– Qwen å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ AsyncOpenAI å…¼å®¹æ¥å£ï¼‰
        self.model = model
        self.base_url = base_url
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            **kwargs  # ç›´æ¥é€ä¼ å…¶ä»–å‚æ•°ç»™ SDK
        )

        # å¯¹è¯çŠ¶æ€
        self.current_conversation = ""  
        self.tool_results: List[ToolResult] = []
        
        # åˆå§‹åŒ– usage ç»Ÿè®¡
        self.usage = Usage(
            input_price=ModelPrices.GPT41_INPUT_PRICE,  # æš‚æ—¶ä½¿ç”¨ç›¸åŒä»·æ ¼ï¼Œå¯ä»¥åç»­è°ƒæ•´
            output_price=ModelPrices.GPT41_OUTPUT_PRICE
        )

    def _convert_mcp_tools_to_openai_format(self, tools: List[Tool]) -> List[Dict[str, Any]]:
        """å°† MCP å·¥å…·æ ¼å¼è½¬æ¢ä¸º OpenAI æ ¼å¼
        
        Args:
            tools: MCP å·¥å…·åˆ—è¡¨
            
        Returns:
            OpenAI æ ¼å¼çš„å·¥å…·åˆ—è¡¨
        """
        openai_tools = []
        for tool in tools:
            openai_tool = {
                "type": "function",
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.input_schema
            }
            openai_tools.append(openai_tool)
        return openai_tools

    def _convert_mcp_tools_to_chat_format(self, tools: List[Tool]) -> List[Dict[str, Any]]:
        """å°† MCP å·¥å…·æ ¼å¼è½¬æ¢ä¸º Chat Completions API æ ¼å¼
        
        Args:
            tools: MCP å·¥å…·åˆ—è¡¨
            
        Returns:
            Chat Completions API æ ¼å¼çš„å·¥å…·åˆ—è¡¨
        """
        chat_tools = []
        for tool in tools:
            chat_tool = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.input_schema
                }
            }
            chat_tools.append(chat_tool)
        return chat_tools

    async def _process_chat_tool_call(self, tool_call: Dict[str, Any]) -> Optional[ToolResult]:
        """å¤„ç†éæµå¼å¯¹è¯ä¸­çš„å·¥å…·è°ƒç”¨
        
        Args:
            tool_call: å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œæ ¼å¼ä¸º chat completions API çš„æ ¼å¼
            
        Returns:
            å·¥å…·è°ƒç”¨ç»“æœ
        """
        tool_name = tool_call["function"]["name"]
        
        # åªå¤„ç† MCP å·¥å…·
        if not self.get_tool_by_name(tool_name):
            print(f"DEBUG: è·³è¿‡é MCP å·¥å…·: {tool_name}")  # è°ƒè¯•ä¿¡æ¯
            return None
            
        tool_input = json.loads(tool_call["function"]["arguments"])
        return await self._call_tool(tool_name, tool_input)
    
    async def _process_stream_tool_call(self, tool_call: Dict[str, Any]) -> Optional[ToolResult]:
        """å¤„ç†æµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨
        
        Args:
            tool_call: å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œæ ¼å¼ä¸º chat completions API çš„æ ¼å¼
            
        Returns:
            å·¥å…·è°ƒç”¨ç»“æœ
        """
        tool_name = tool_call["function"]["name"]
        
        # åªå¤„ç† MCP å·¥å…·
        if not self.get_tool_by_name(tool_name):
            print(f"DEBUG: è·³è¿‡é MCP å·¥å…·: {tool_name}")  # è°ƒè¯•ä¿¡æ¯
            return None
            
        tool_input = json.loads(tool_call["function"]["arguments"])
        return await self._call_tool(tool_name, tool_input)
    
    async def _call_tool(self, tool_name: str, tool_input: Dict[str, Any]) -> Optional[ToolResult]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥å‚æ•°
            
        Returns:
            å·¥å…·è°ƒç”¨ç»“æœ
        """
        try:
            print(f"DEBUG: å¼€å§‹è°ƒç”¨å·¥å…· {tool_name}ï¼Œå‚æ•°: {tool_input}")
            result = await self.call_mcp_tool(
                tool_name=tool_name,
                params=tool_input
            )
            print(f"DEBUG: å·¥å…· {tool_name} è°ƒç”¨æˆåŠŸï¼Œç»“æœ: {result}")
            
            # ä¿å­˜å·¥å…·è°ƒç”¨ç»“æœ
            tool_result = ToolResult(tool_name, result)
            self.tool_results.append(tool_result)
            return tool_result
            
        except Exception as e:
            print(f"ERROR: å·¥å…·è°ƒç”¨æœ€ç»ˆå¤±è´¥ (æ‰€æœ‰é‡è¯•éƒ½ç”¨å®Œ)")
            print(f"  å·¥å…·åç§°: {tool_name}")
            print(f"  è¾“å…¥å‚æ•°: {tool_input}")
            print(f"  æœ€ç»ˆå¼‚å¸¸: {type(e).__name__}: {str(e)}")
            return None

    def _format_assistant_content(self, content: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–åŠ©æ‰‹çš„å›å¤å†…å®¹
        
        Args:
            content: åŠ©æ‰‹çš„å›å¤å†…å®¹åˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–åçš„å†…å®¹
        """
        formatted_content = ""
        for item in content:
            if item["role"] == "assistant":
                if item.get("content"):
                    formatted_content += item["content"]
                elif item.get("tool_calls"):
                    # OpenAI çš„å·¥å…·è°ƒç”¨ä¸ä¼šè¿”å›å¯¹è¯å†…å®¹
                    pass
        return formatted_content

    def _generate_json_format_prompt(self, schema: Dict[str, Any]) -> str:
        """æ ¹æ®JSON schemaç”Ÿæˆæ ¼å¼è¦æ±‚çš„prompt
        
        Args:
            schema: JSON schemaå®šä¹‰
            
        Returns:
            æ ¼å¼è¦æ±‚çš„promptæ–‡æœ¬
        """
        if not schema or schema.get("type") != "json_schema":
            return ""
        
        json_schema = schema.get("json_schema", {})
        schema_def = json_schema.get("schema", {})
        
        # ç”Ÿæˆç¤ºä¾‹JSONç»“æ„
        def generate_example_json(schema_obj: Dict[str, Any], depth: int = 0) -> str:
            if depth > 3:  # é˜²æ­¢æ— é™é€’å½’
                return '"..."'
            
            schema_type = schema_obj.get("type", "string")
            properties = schema_obj.get("properties", {})
            required = schema_obj.get("required", [])
            
            if schema_type == "object" and properties:
                lines = ["{"]
                for i, (key, prop_schema) in enumerate(properties.items()):
                    indent = "  " * (depth + 1)
                    value = generate_example_json(prop_schema, depth + 1)
                    comma = "," if i < len(properties) - 1 else ""
                    lines.append(f'{indent}"{key}": {value}{comma}')
                lines.append("  " * depth + "}")
                return "\n".join(lines)
            
            elif schema_type == "array":
                items_schema = schema_obj.get("items", {"type": "string"})
                item_example = generate_example_json(items_schema, depth + 1)
                return f"[\n{'  ' * (depth + 1)}{item_example}\n{'  ' * depth}]"
            
            elif schema_type == "string":
                description = schema_obj.get("description", "å­—ç¬¦ä¸²å€¼")
                return f'"{description}"'
            
            elif schema_type == "number" or schema_type == "integer":
                return "0"
            
            elif schema_type == "boolean":
                return "true"
            
            else:
                return '"å€¼"'
        
        try:
            example_json = generate_example_json(schema_def)
            
            # æå–æ‰€æœ‰å¿…éœ€å­—æ®µ
            def extract_required_fields(obj: Dict[str, Any], path: str = "") -> List[str]:
                fields = []
                if obj.get("type") == "object":
                    properties = obj.get("properties", {})
                    required = obj.get("required", [])
                    for field in required:
                        field_path = f"{path}.{field}" if path else field
                        fields.append(field_path)
                        # é€’å½’å¤„ç†åµŒå¥—å¯¹è±¡
                        if field in properties:
                            nested_fields = extract_required_fields(properties[field], field_path)
                            fields.extend(nested_fields)
                return fields
            
            required_fields = extract_required_fields(schema_def)
            
            format_prompt = f"""\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{example_json}

âš ï¸ é‡è¦æç¤ºï¼š
1. å¿…é¡»ä¸¥æ ¼ä½¿ç”¨ä¸Šè¿°JSONä¸­çš„å­—æ®µåï¼Œä¸å¾—æ›´æ”¹æˆ–æ›¿æ¢
2. å¿…é¡»åŒ…å«ä»¥ä¸‹æ‰€æœ‰å¿…éœ€å­—æ®µï¼š{', '.join(required_fields)}
3. å­—æ®µåå¿…é¡»å®Œå…¨ä¸€è‡´ï¼ŒåŒ…æ‹¬å¤§å°å†™
4. è¯·å‹¿æ·»åŠ schemaä¸­æœªå®šä¹‰çš„é¢å¤–å­—æ®µ
5. ç¡®ä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼"""
            
            return format_prompt
        except Exception as e:
            print(f"DEBUG: ç”ŸæˆJSONæ ¼å¼promptæ—¶å‡ºé”™: {e}")
            # å›é€€åˆ°ç®€å•æç¤º
            return "\n\nè¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚"

    def _enhance_content_with_json_format(self, content: str, **kwargs) -> str:
        """å¢å¼ºcontentï¼Œå¦‚æœæœ‰response_formatåˆ™è‡ªåŠ¨æ·»åŠ JSONæ ¼å¼è¦æ±‚
        
        Args:
            content: åŸå§‹content
            **kwargs: å¯èƒ½åŒ…å«response_formatçš„å‚æ•°
            
        Returns:
            å¢å¼ºåçš„content
        """
        response_format = kwargs.get("response_format")
        if not response_format:
            return content
        
        # æ£€æŸ¥contentä¸­æ˜¯å¦å·²ç»åŒ…å«"json"æˆ–"JSON"
        needs_json_keyword = "json" not in content.lower()
        
        # ç”Ÿæˆå…·ä½“çš„æ ¼å¼è¦æ±‚
        format_prompt = self._generate_json_format_prompt(response_format)
        
        if needs_json_keyword:
            # å¦‚æœæ²¡æœ‰åŒ…å«jsonå­—çœ¼ï¼Œå…ˆæ·»åŠ åŸºæœ¬è¦æ±‚ï¼Œå†æ·»åŠ è¯¦ç»†æ ¼å¼
            enhanced_content = content + "\n\nè¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚" + format_prompt
        else:
            # å¦‚æœå·²ç»åŒ…å«jsonå­—çœ¼ï¼Œåªæ·»åŠ è¯¦ç»†æ ¼å¼è¦æ±‚
            enhanced_content = content + format_prompt
        
        print(f"DEBUG: è‡ªåŠ¨å¢å¼ºpromptï¼Œæ·»åŠ äº†JSONæ ¼å¼è¦æ±‚")
        return enhanced_content

    @async_retry(timeout=60.0)
    async def chat(self, content: str, **kwargs) -> Dict[str, Any]:
        """å¯¹è¯
        
        Args:
            content: å½“å‰è½®æ¬¡çš„å¯¹è¯å†…å®¹
            
        Returns:
            å¯¹è¯å“åº”
        """
        print(f"DEBUG: chatå¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {content}")  # è°ƒè¯•ä¿¡æ¯
        
        # ğŸš€ è‡ªåŠ¨å¢å¼ºpromptä»¥æ”¯æŒç»“æ„åŒ–è¾“å‡º
        enhanced_content = self._enhance_content_with_json_format(content, **kwargs)
        if enhanced_content != content:
            print(f"DEBUG: promptå·²è‡ªåŠ¨å¢å¼ºä»¥æ”¯æŒJSONæ ¼å¼è¾“å‡º")
        
        # æ›´æ–°å½“å‰å¯¹è¯å†…å®¹
        if self.current_conversation:
            self.current_conversation += f"\nUser: {enhanced_content}\n"
        else:
            self.current_conversation = f"User: {enhanced_content}\n"
        
        print(f"DEBUG: å½“å‰å¯¹è¯å†…å®¹:\n{self.current_conversation}")  # è°ƒè¯•ä¿¡æ¯
        
        # å¾ªç¯å¤„ç†ï¼Œç›´åˆ°æ²¡æœ‰å·¥å…·è°ƒç”¨
        while True:
            # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
            tools = self.get_available_tools()
            chat_tools = self._convert_mcp_tools_to_chat_format(tools)
            print(f"DEBUG: å¯ç”¨å·¥å…·æ•°é‡: {len(tools)}")  # è°ƒè¯•ä¿¡æ¯
            
            # åˆå¹¶ç”¨æˆ·ä¼ å…¥çš„å·¥å…·å’Œ MCP å·¥å…·
            if 'tools' in kwargs:
                user_tools = kwargs.pop('tools')
                all_tools = chat_tools + user_tools
            else:
                all_tools = chat_tools
            
            print(f"DEBUG: å‡†å¤‡è°ƒç”¨chat completions API...")  # è°ƒè¯•ä¿¡æ¯
            
            # è°ƒç”¨chat completions API
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": self.current_conversation}
                ],
                tools=all_tools if all_tools else None,  # å¦‚æœæ²¡æœ‰å·¥å…·å°±ä¸ä¼ toolså‚æ•°
                **kwargs
            )
            
            response_data = response.model_dump()
            print(f"DEBUG: æ”¶åˆ°APIå“åº”")  # è°ƒè¯•ä¿¡æ¯
            
            # æ›´æ–°usageç»Ÿè®¡
            if usage := response_data.get("usage"):
                self.usage.input_tokens += usage.get("prompt_tokens", 0)
                self.usage.output_tokens += usage.get("completion_tokens", 0)
                print(f"DEBUG: æ›´æ–°usage - è¾“å…¥:{usage.get('prompt_tokens', 0)}, è¾“å‡º:{usage.get('completion_tokens', 0)}")
            
            message = response_data["choices"][0]["message"]
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            has_tool_calls = False
            if message.get("tool_calls"):
                print(f"DEBUG: å‘ç°{len(message['tool_calls'])}ä¸ªå·¥å…·è°ƒç”¨")  # è°ƒè¯•ä¿¡æ¯
                
                # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                assistant_content = message.get("content", "")
                if assistant_content:
                    self.current_conversation += f"Assistant: {assistant_content}\n"
                
                # å¤„ç†æ¯ä¸ªå·¥å…·è°ƒç”¨
                for tool_call in message["tool_calls"]:
                    result = await self._process_chat_tool_call(tool_call)
                    if result:
                        has_tool_calls = True
                        # æ·»åŠ å·¥å…·è°ƒç”¨ç»“æœåˆ°å¯¹è¯å†…å®¹
                        tool_response = f"Tool <{result.tool_name}> returned: {result.tool_result}\n"
                        self.current_conversation += tool_response
                        print(f"DEBUG: å·¥å…·è°ƒç”¨ç»“æœå·²æ·»åŠ åˆ°å¯¹è¯")
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åŠ©æ‰‹å›å¤åˆ°å¯¹è¯å†å²
                assistant_content = message.get("content", "")
                if assistant_content:
                    self.current_conversation += f"Assistant: {assistant_content}\n"
                print(f"DEBUG: æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ")
            
            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›å“åº”
            if not has_tool_calls:
                print(f"DEBUG: chatå‡½æ•°å®Œæˆ")
                return response_data
            else:
                print(f"DEBUG: ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ")
                # ç»§ç»­å¾ªç¯å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ

    def _convert_chat_chunk_to_response_format(self, chunk_data: Dict[str, Any], chunk_index: int = 0) -> Dict[str, Any]:
        """å°†Chat Completions APIçš„chunkè½¬æ¢ä¸ºç±»ä¼¼Responses APIçš„æ ¼å¼
        
        Args:
            chunk_data: Chat APIçš„åŸå§‹chunkæ•°æ®
            chunk_index: chunkç´¢å¼•
            
        Returns:
            è½¬æ¢åçš„å“åº”æ ¼å¼
        """
        if not chunk_data.get("choices"):
            # å¦‚æœæ²¡æœ‰choicesï¼Œè¿”å›åŸå§‹æ•°æ®
            return chunk_data
        
        choice = chunk_data["choices"][0]
        delta = choice.get("delta", {})
        finish_reason = choice.get("finish_reason")
        
        # æ ¹æ®å†…å®¹ç±»å‹ç”Ÿæˆä¸åŒçš„å“åº”æ ¼å¼
        if delta.get("content"):
            # æ–‡æœ¬å†…å®¹å¢é‡
            return {
                "type": "response.function_call_arguments.delta",
                "delta": delta["content"]
            }
        
        elif delta.get("tool_calls"):
            # å·¥å…·è°ƒç”¨å¢é‡å¤„ç†
            tool_call = delta["tool_calls"][0]  # é€šå¸¸åªæœ‰ä¸€ä¸ªå·¥å…·è°ƒç”¨
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨å¼€å§‹ï¼ˆæœ‰å·¥å…·IDå’Œåç§°ï¼‰
            if tool_call.get("id") and tool_call.get("function", {}).get("name"):
                return {
                    "type": "response.tool_call_start",
                    "tool_call_id": tool_call["id"],
                    "tool_name": tool_call["function"]["name"]
                }
            # æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°å¢é‡
            elif tool_call.get("function", {}).get("arguments"):
                return {
                    "type": "response.function_call_arguments.delta",
                    "delta": tool_call["function"]["arguments"]
                }
            else:
                # å…¶ä»–å·¥å…·è°ƒç”¨ç›¸å…³çš„å¢é‡
                return {
                    "type": "response.function_call_arguments.delta",
                    "delta": ""
                }
        
        elif finish_reason == "tool_calls":
            # å·¥å…·è°ƒç”¨å®Œæˆ
            return {
                "type": "response.function_call_arguments.done"
            }
        
        elif finish_reason:
            # æ™®é€šå“åº”å®Œæˆ
            return {
                "type": "response.completed",
                "response": {
                    "output": [{
                        "type": "message",
                        "status": "completed",
                        "content": [{"type": "text", "text": ""}]  # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦å¡«å……
                    }],
                    "usage": chunk_data.get("usage")
                }
            }
        
        elif chunk_index == 0:
            # ç¬¬ä¸€ä¸ªchunkï¼Œè¡¨ç¤ºå“åº”å¼€å§‹
            return {
                "type": "response.created"
            }
        
        else:
            # å…¶ä»–æƒ…å†µï¼Œè¿”å›è¿›è¡Œä¸­çŠ¶æ€
            return {
                "type": "response.in_progress"
            }

    @stream_async_retry(max_retries=3, chunk_timeout=60.0)
    async def stream_chat(self, content: str, **kwargs) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼å¯¹è¯å’Œå·¥å…·è°ƒç”¨å¤„ç†
        
        æ³¨æ„ï¼šæµå¼æ¥å£é€šå¸¸ä¸æä¾›å‡†ç¡®çš„tokenç»Ÿè®¡ä¿¡æ¯ï¼Œå› æ­¤usage.input_tokenså’Œusage.output_tokens
        åœ¨ä½¿ç”¨æµå¼æ¨¡å¼æ—¶å°†æ— æ³•æ­£ç¡®ç»Ÿè®¡ã€‚å¦‚éœ€å‡†ç¡®çš„tokenç»Ÿè®¡ï¼Œè¯·ä½¿ç”¨éæµå¼çš„chat()æ–¹æ³•ã€‚
        
        Args:
            content: å½“å‰è½®æ¬¡çš„å¯¹è¯å†…å®¹
            
        Yields:
            æµå¼å“åº”çš„ chunksï¼ˆè½¬æ¢ä¸ºç±»ä¼¼OpenAI Responses APIçš„æ ¼å¼ï¼‰
        """
        print(f"DEBUG: stream_chatå¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {content}")  # è°ƒè¯•ä¿¡æ¯
        
        # ğŸš€ è‡ªåŠ¨å¢å¼ºpromptä»¥æ”¯æŒç»“æ„åŒ–è¾“å‡º
        enhanced_content = self._enhance_content_with_json_format(content, **kwargs)
        if enhanced_content != content:
            print(f"DEBUG: promptå·²è‡ªåŠ¨å¢å¼ºä»¥æ”¯æŒJSONæ ¼å¼è¾“å‡º")
        
        # æ›´æ–°å½“å‰å¯¹è¯å†…å®¹
        if self.current_conversation:
            self.current_conversation += f"\nUser: {enhanced_content}\n"
        else:
            self.current_conversation = f"User: {enhanced_content}\n"
            
        print(f"DEBUG: å½“å‰å¯¹è¯å†…å®¹:\n{self.current_conversation}")  # è°ƒè¯•ä¿¡æ¯
        
        while True:
            print(f"DEBUG: å½“è½®å‘é€æ¶ˆæ¯: {self.current_conversation}")
            # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
            tools = self.get_available_tools()
            chat_tools = self._convert_mcp_tools_to_chat_format(tools)
            print(f"DEBUG: å¯ç”¨å·¥å…·æ•°é‡: {len(tools)}")  # è°ƒè¯•ä¿¡æ¯
            
            # åˆå¹¶ç”¨æˆ·ä¼ å…¥çš„å·¥å…·å’Œ MCP å·¥å…·
            if 'tools' in kwargs:
                user_tools = kwargs.pop('tools')
                all_tools = chat_tools + user_tools
            else:
                all_tools = chat_tools
            
            print("DEBUG: å‡†å¤‡åˆ›å»ºæµå¼ä¼šè¯...")  # è°ƒè¯•ä¿¡æ¯
            
            # å­˜å‚¨å·¥å…·è°ƒç”¨ä¿¡æ¯
            accumulated_tool_calls = {}
            collected_content = ""
            final_response = None
            chunk_index = 0
            
            try:
                # ä½¿ç”¨ Chat Completions API çš„æµå¼æ¨¡å¼
                stream = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": self.current_conversation}],
                    tools=all_tools if all_tools else None,
                    stream=True,
                    **kwargs
                )
                
                print("DEBUG: æµå¼ä¼šè¯åˆ›å»ºæˆåŠŸ")  # è°ƒè¯•ä¿¡æ¯
                
                # å¤„ç†æµå¼å“åº”
                print("DEBUG: å¼€å§‹å¤„ç†æµå¼å“åº”...")  # è°ƒè¯•ä¿¡æ¯
                async for chunk in stream:
                    chunk_data = chunk.model_dump()
                    
                    # ğŸš€ è½¬æ¢ä¸ºæ ‡å‡†åŒ–æ ¼å¼å¹¶è¿”å›ç»™è°ƒç”¨è€…
                    standardized_chunk = self._convert_chat_chunk_to_response_format(chunk_data, chunk_index)
                    yield standardized_chunk
                    chunk_index += 1
                    
                    # æ”¶é›†å®Œæ•´å“åº”æ•°æ®
                    if chunk_data.get("choices"):
                        choice = chunk_data["choices"][0]
                        delta = choice.get("delta", {})
                        
                        # æ”¶é›†å†…å®¹
                        if delta.get("content"):
                            collected_content += delta["content"]
                        
                        # æ”¶é›†å·¥å…·è°ƒç”¨
                        if delta.get("tool_calls"):
                            for tool_call in delta["tool_calls"]:
                                tool_id = tool_call.get("id")
                                if tool_id:
                                    # åˆå§‹åŒ–å·¥å…·è°ƒç”¨è®°å½•
                                    if tool_id not in accumulated_tool_calls:
                                        accumulated_tool_calls[tool_id] = {
                                            "id": tool_id,
                                            "type": tool_call.get("type", "function"),
                                            "function": {
                                                "name": "",
                                                "arguments": ""
                                            }
                                        }
                                    
                                    # ç´¯ç§¯å·¥å…·è°ƒç”¨ä¿¡æ¯
                                    if tool_call.get("function"):
                                        func = tool_call["function"]
                                        if func.get("name"):
                                            accumulated_tool_calls[tool_id]["function"]["name"] = func["name"]
                                        if func.get("arguments"):
                                            accumulated_tool_calls[tool_id]["function"]["arguments"] += func["arguments"]
                                else:
                                    # å¤„ç†æ²¡æœ‰IDçš„æƒ…å†µï¼ˆä½¿ç”¨indexä½œä¸ºä¸´æ—¶IDï¼‰
                                    tool_index = tool_call.get("index", 0)
                                    temp_id = f"temp_{tool_index}"
                                    
                                    if temp_id not in accumulated_tool_calls:
                                        accumulated_tool_calls[temp_id] = {
                                            "id": temp_id,
                                            "type": tool_call.get("type", "function"),
                                            "function": {
                                                "name": "",
                                                "arguments": ""
                                            }
                                        }
                                    
                                    if tool_call.get("function"):
                                        func = tool_call["function"]
                                        if func.get("name"):
                                            accumulated_tool_calls[temp_id]["function"]["name"] = func["name"]
                                        if func.get("arguments"):
                                            accumulated_tool_calls[temp_id]["function"]["arguments"] += func["arguments"]
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªchunk
                        if choice.get("finish_reason"):
                            final_response = {
                                "choices": [{
                                    "message": {
                                        "role": "assistant",
                                        "content": collected_content,
                                        "tool_calls": list(accumulated_tool_calls.values()) if accumulated_tool_calls else None
                                    },
                                    "finish_reason": choice["finish_reason"]
                                }],
                                "usage": chunk_data.get("usage")
                            }
                    
                    # æ³¨æ„ï¼šæµå¼æ¥å£é€šå¸¸ä¸æä¾›å‡†ç¡®çš„tokenç»Ÿè®¡ä¿¡æ¯
                    # å› æ­¤åœ¨æµå¼æ¨¡å¼ä¸‹ä¸è¿›è¡Œtokenç»Ÿè®¡
                    # if chunk_data.get("usage"):
                    #     usage = chunk_data["usage"]
                    #     self.usage.input_tokens += usage.get("prompt_tokens", 0)
                    #     self.usage.output_tokens += usage.get("completion_tokens", 0)
                
                print("DEBUG: æµå¼å“åº”å¤„ç†å®Œæˆ")  # è°ƒè¯•ä¿¡æ¯
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                has_tool_calls = False
                if final_response and accumulated_tool_calls:
                    print(f"DEBUG: å‘ç°{len(accumulated_tool_calls)}ä¸ªå·¥å…·è°ƒç”¨")
                    
                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å¯¹è¯å†å²
                    if collected_content:
                        self.current_conversation += f"Assistant: {collected_content}\n"
                    
                    # å¤„ç†æ¯ä¸ªå·¥å…·è°ƒç”¨
                    for tool_call in accumulated_tool_calls.values():
                        result = await self._process_stream_tool_call(tool_call)
                        if result:
                            has_tool_calls = True
                            # æ·»åŠ å·¥å…·è°ƒç”¨ç»“æœåˆ°å¯¹è¯å†…å®¹
                            self.current_conversation += f"Tool <{result.tool_name}> Result Returned: {result.tool_result}\n"
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åŠ©æ‰‹å›å¤åˆ°å¯¹è¯å†å²
                    if collected_content:
                        self.current_conversation += f"Assistant: {collected_content}\n"
                
                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œé€€å‡ºå¾ªç¯
                if not has_tool_calls:
                    print("DEBUG: æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæµå¼å¯¹è¯ç»“æŸ")
                    break
                else:
                    print("DEBUG: æœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­ä¸‹ä¸€è½®æµå¼å¯¹è¯å¤„ç†å·¥å…·ç»“æœ")
                    # ç»§ç»­whileå¾ªç¯ï¼Œè¿›è¡Œä¸‹ä¸€è½®æµå¼å¯¹è¯
                    
            except Exception as e:
                print(f"ERROR: æµå¼å¤„ç†å¼‚å¸¸: {str(e)}")  # é”™è¯¯ä¿¡æ¯
                raise 