"""OpenAI API å®¢æˆ·ç«¯"""
import json
from dataclasses import dataclass
from openai import AsyncOpenAI
from typing import Dict, Any, AsyncIterator, List, Optional
from base_client import BaseLLMClient, Tool, Usage, ModelPrices
from utils.retry import async_retry, stream_async_retry

@dataclass
class ToolResult:
    """å·¥å…·è°ƒç”¨ç»“æœ
    
    Args:
        tool_name: å·¥å…·åç§°
        tool_result: å·¥å…·è°ƒç”¨ç»“æœ
    """
    tool_name: str
    tool_result: Any

class OpenAIClient(BaseLLMClient):
    """OpenAI API å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4.1",
        **kwargs
    ):
        # å°† LLM ç›¸å…³å‚æ•°ä» kwargs ä¸­åˆ†ç¦»å‡ºæ¥
        mcp_kwargs = {}
        if 'mcp_urls' in kwargs:
            mcp_kwargs['mcp_urls'] = kwargs.pop('mcp_urls')
        if 'enable_timeout_retry' in kwargs:
            mcp_kwargs['enable_timeout_retry'] = kwargs.pop('enable_timeout_retry')
            
        super().__init__(api_key, **mcp_kwargs)
        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        self.model = model
        self.client = AsyncOpenAI(
            api_key=api_key,
            **kwargs  # ç›´æ¥é€ä¼ å…¶ä»–å‚æ•°ç»™ SDK
        )

        # å¯¹è¯çŠ¶æ€
        self.current_conversation = ""  
        self.tool_results: List[ToolResult] = []
        
        # åˆå§‹åŒ– usage ç»Ÿè®¡
        self.usage = Usage(
            input_price=ModelPrices.GPT41_INPUT_PRICE,
            output_price=ModelPrices.GPT41_OUTPUT_PRICE
        )

    def _convert_mcp_tools_to_openai_format(self, tools: List[Tool]) -> List[Dict[str, Any]]:
        """å°† MCP å·¥å…·æ ¼å¼è½¬æ¢ä¸º OpenAI æ ¼å¼
        
        Args:
            tools: MCP å·¥å…·åˆ—è¡¨
            
        Returns:
            OpenAI æ ¼å¼çš„å·¥å…·åˆ—è¡¨
        """
        openai_tools = []
        for tool in tools:
            openai_tool = {
                "type": "function",
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.input_schema
            }
            openai_tools.append(openai_tool)
        return openai_tools

    def _convert_mcp_tools_to_chat_format(self, tools: List[Tool]) -> List[Dict[str, Any]]:
        """å°† MCP å·¥å…·æ ¼å¼è½¬æ¢ä¸º Chat Completions API æ ¼å¼
        
        Args:
            tools: MCP å·¥å…·åˆ—è¡¨
            
        Returns:
            Chat Completions API æ ¼å¼çš„å·¥å…·åˆ—è¡¨
        """
        chat_tools = []
        for tool in tools:
            chat_tool = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.input_schema
                }
            }
            chat_tools.append(chat_tool)
        return chat_tools

    async def _process_response_tool_call(self, output: Dict[str, Any]) -> Optional[ToolResult]:
        """å¤„ç†éæµå¼ Response API ä¸­çš„å·¥å…·è°ƒç”¨

        Args:
            output: å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œæ ¼å¼ä¸º Response API çš„æ ¼å¼

        Returns:
            å·¥å…·è°ƒç”¨ç»“æœ
        """
        if output.get("type") != "function_call" or output.get("status") != "completed":
            return None

        tool_name = output.get("name")
        # åªå¤„ç† MCP å·¥å…·
        if not self.get_tool_by_name(tool_name):
            print(f"DEBUG: è·³è¿‡é MCP å·¥å…·: {tool_name}")  # è°ƒè¯•ä¿¡æ¯
            return None

        tool_input = json.loads(output.get("arguments", "{}"))
        return await self._call_tool(tool_name, tool_input)
    
    async def _process_stream_tool_call(self, output: Dict[str, Any]) -> Optional[ToolResult]:
        """å¤„ç†æµå¼å“åº”ä¸­çš„å·¥å…·è°ƒç”¨
        
        Args:
            output: å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œæ ¼å¼ä¸º response API çš„æ ¼å¼
            
        Returns:
            å·¥å…·è°ƒç”¨ç»“æœ
        """
        if output.get("type") != "function_call" or output.get("status") != "completed":
            return None
            
        tool_name = output.get("name")
        # åªå¤„ç† MCP å·¥å…·
        if not self.get_tool_by_name(tool_name):
            print(f"DEBUG: è·³è¿‡é MCP å·¥å…·: {tool_name}")  # è°ƒè¯•ä¿¡æ¯
            return None
            
        tool_input = json.loads(output.get("arguments", "{}"))
        return await self._call_tool(tool_name, tool_input)
    
    async def _call_tool(self, tool_name: str, tool_input: Dict[str, Any]) -> Optional[ToolResult]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨
        
        Args:
            tool_name: å·¥å…·åç§°
            tool_input: å·¥å…·è¾“å…¥å‚æ•°
            
        Returns:
            å·¥å…·è°ƒç”¨ç»“æœ
        """
        try:
            print(f"DEBUG: å¼€å§‹è°ƒç”¨å·¥å…· {tool_name}ï¼Œå‚æ•°: {tool_input}")
            result = await self.call_mcp_tool(
                tool_name=tool_name,
                params=tool_input
            )
            print(f"DEBUG: å·¥å…· {tool_name} è°ƒç”¨æˆåŠŸï¼Œç»“æœ: {result}")
            
            # ä¿å­˜å·¥å…·è°ƒç”¨ç»“æœ
            tool_result = ToolResult(tool_name, result)
            self.tool_results.append(tool_result)
            return tool_result
            
        except Exception as e:
            print(f"ERROR: å·¥å…·è°ƒç”¨æœ€ç»ˆå¤±è´¥ (æ‰€æœ‰é‡è¯•éƒ½ç”¨å®Œ)")
            print(f"  å·¥å…·åç§°: {tool_name}")
            print(f"  è¾“å…¥å‚æ•°: {tool_input}")
            print(f"  æœ€ç»ˆå¼‚å¸¸: {type(e).__name__}: {str(e)}")
            return None

    def _extract_text_from_response_output(self, outputs: List[Dict[str, Any]]) -> str:
        """ä» Response API çš„ output ä¸­æå–æ–‡æœ¬å†…å®¹

        Args:
            outputs: Response API è¿”å›çš„ output åˆ—è¡¨

        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        text_content = ""
        for output in outputs:
            if output.get("type") == "message":
                content_blocks = output.get("content", [])
                for block in content_blocks:
                    if block.get("type") == "text":
                        text_content += block.get("text", "")
        return text_content

    @async_retry(timeout=60.0)
    async def chat(self, content: str, **kwargs) -> Dict[str, Any]:
        """å¯¹è¯ - ä½¿ç”¨ Response APIï¼ˆæ— çŠ¶æ€æ¨¡å¼ï¼‰

        Args:
            content: å½“å‰è½®æ¬¡çš„å¯¹è¯å†…å®¹

        Returns:
            å¯¹è¯å“åº”
        """
        print(f"DEBUG: chatå¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {content}")  # è°ƒè¯•ä¿¡æ¯

        # æ›´æ–°å½“å‰å¯¹è¯å†…å®¹ï¼ˆå®¢æˆ·ç«¯ç®¡ç†çŠ¶æ€ï¼‰
        if self.current_conversation:
            self.current_conversation += f"\nUser: {content}\n"
        else:
            self.current_conversation = f"User: {content}\n"

        print(f"DEBUG: å½“å‰å¯¹è¯å†…å®¹:\n{self.current_conversation}")  # è°ƒè¯•ä¿¡æ¯

        # å¾ªç¯å¤„ç†ï¼Œç›´åˆ°æ²¡æœ‰å·¥å…·è°ƒç”¨
        while True:
            # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
            tools = self.get_available_tools()
            mcp_tools = self._convert_mcp_tools_to_openai_format(tools)
            print(f"DEBUG: å¯ç”¨å·¥å…·æ•°é‡: {len(tools)}")  # è°ƒè¯•ä¿¡æ¯

            # åˆå¹¶ç”¨æˆ·ä¼ å…¥çš„å·¥å…·å’Œ MCP å·¥å…·
            if 'tools' in kwargs:
                user_tools = kwargs.pop('tools')
                all_tools = mcp_tools + user_tools
            else:
                all_tools = mcp_tools

            print(f"DEBUG: å‡†å¤‡è°ƒç”¨ Response APIï¼ˆæ— çŠ¶æ€æ¨¡å¼ï¼‰...")  # è°ƒè¯•ä¿¡æ¯

            # è°ƒç”¨ Response APIï¼ˆæ— çŠ¶æ€æ¨¡å¼ï¼šstore=Falseï¼‰
            response = await self.client.responses.create(
                model=self.model,
                input=[{"role": "user", "content": self.current_conversation}],
                tools=all_tools if all_tools else None,
                store=False,  # ğŸ”‘ å…³é”®ï¼šä¸ä½¿ç”¨æœåŠ¡ç«¯çŠ¶æ€ç®¡ç†ï¼Œä¿æŒå®¢æˆ·ç«¯ç®¡ç†
                **kwargs
            )

            response_data = response.model_dump()
            print(f"DEBUG: æ”¶åˆ° Response API å“åº”")  # è°ƒè¯•ä¿¡æ¯

            # æ›´æ–°usageç»Ÿè®¡ï¼ˆResponse API ä½¿ç”¨ input_tokens/output_tokensï¼‰
            if usage := response_data.get("usage"):
                self.usage.input_tokens += usage.get("input_tokens", 0)
                self.usage.output_tokens += usage.get("output_tokens", 0)
                print(f"DEBUG: æ›´æ–°usage - è¾“å…¥:{usage.get('input_tokens', 0)}, è¾“å‡º:{usage.get('output_tokens', 0)}")

            # å¤„ç† Response API çš„è¾“å‡ºæ ¼å¼
            outputs = response_data.get("output", [])

            # æå–æ–‡æœ¬å†…å®¹
            assistant_content = self._extract_text_from_response_output(outputs)

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            has_tool_calls = False
            for output in outputs:
                if output.get("type") == "function_call":
                    print(f"DEBUG: å‘ç°å·¥å…·è°ƒç”¨: {output.get('name')}")  # è°ƒè¯•ä¿¡æ¯
                    result = await self._process_response_tool_call(output)
                    if result:
                        has_tool_calls = True
                        # æ·»åŠ å·¥å…·è°ƒç”¨ç»“æœåˆ°å¯¹è¯å†…å®¹
                        tool_response = f"Tool <{result.tool_name}> returned: {result.tool_result}\n"
                        self.current_conversation += tool_response
                        print(f"DEBUG: å·¥å…·è°ƒç”¨ç»“æœå·²æ·»åŠ åˆ°å¯¹è¯")

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å¯¹è¯å†å²
            if assistant_content:
                self.current_conversation += f"Assistant: {assistant_content}\n"

            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›å“åº”
            if not has_tool_calls:
                print(f"DEBUG: æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ")
                print(f"DEBUG: chatå‡½æ•°å®Œæˆ")
                return response_data
            else:
                print(f"DEBUG: ç»§ç»­ä¸‹ä¸€è½®å¯¹è¯å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ")
                # ç»§ç»­å¾ªç¯å¤„ç†å·¥å…·è°ƒç”¨ç»“æœ

    @stream_async_retry(max_retries=3, chunk_timeout=60.0)
    async def stream_chat(self, content: str, **kwargs) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼å¯¹è¯å’Œå·¥å…·è°ƒç”¨å¤„ç†
        
        Args:
            content: å½“å‰è½®æ¬¡çš„å¯¹è¯å†…å®¹
            
        Yields:
            æµå¼å“åº”çš„ chunks
        """
        print(f"DEBUG: stream_chatå¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: {content}")  # è°ƒè¯•ä¿¡æ¯
        
        # æ›´æ–°å½“å‰å¯¹è¯å†…å®¹
        if self.current_conversation:
            self.current_conversation += f"\nUser: {content}\n"
        else:
            self.current_conversation = f"User: {content}\n"
            
        print(f"DEBUG: å½“å‰å¯¹è¯å†…å®¹:\n{self.current_conversation}")  # è°ƒè¯•ä¿¡æ¯
        
        while True:
            print(f"DEBUG: å½“è½®å‘é€æ¶ˆæ¯: {self.current_conversation}")
            # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
            tools = self.get_available_tools()
            mcp_tools = self._convert_mcp_tools_to_openai_format(tools)
            print(f"DEBUG: å¯ç”¨å·¥å…·æ•°é‡: {len(tools)}")  # è°ƒè¯•ä¿¡æ¯
            
            # åˆå¹¶ç”¨æˆ·ä¼ å…¥çš„å·¥å…·å’Œ MCP å·¥å…·
            if 'tools' in kwargs:
                user_tools = kwargs.pop('tools')
                all_tools = mcp_tools + user_tools
            else:
                all_tools = mcp_tools
            
            print("DEBUG: å‡†å¤‡åˆ›å»ºæµå¼ä¼šè¯...")  # è°ƒè¯•ä¿¡æ¯
            print(all_tools)  # è°ƒè¯•ä¿¡æ¯
            try:
                # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ›å»ºæµå¼ä¼šè¯ï¼ˆæ— çŠ¶æ€æ¨¡å¼ï¼‰
                async with await self.client.responses.create(
                    model=self.model,
                    input=[{"role": "user", "content": self.current_conversation}],
                    tools=all_tools if all_tools else None,
                    store=False,  # ğŸ”‘ å…³é”®ï¼šä¸ä½¿ç”¨æœåŠ¡ç«¯çŠ¶æ€ç®¡ç†ï¼Œä¿æŒå®¢æˆ·ç«¯ç®¡ç†
                    stream=True,
                    **kwargs
                ) as stream:
                    print("DEBUG: æµå¼ä¼šè¯åˆ›å»ºæˆåŠŸ")  # è°ƒè¯•ä¿¡æ¯
                    
                    # å¤„ç†æµå¼å“åº”
                    print("DEBUG: å¼€å§‹å¤„ç†æµå¼å“åº”...")  # è°ƒè¯•ä¿¡æ¯
                    final_message = None
                    async for chunk in stream:
                        chunk_data = chunk.model_dump()
                        
                        # å¦‚æœæ˜¯æœ€åä¸€ä¸ªå®Œæ•´çš„æ¶ˆæ¯ï¼Œä¿å­˜ä¸‹æ¥
                        if chunk_data.get("type") == "response.completed":
                            final_message = chunk_data.get("response")
                            # æ›´æ–° usage ç»Ÿè®¡
                            if usage := final_message.get("usage"):
                                self.usage.input_tokens += usage.get("input_tokens", 0)
                                self.usage.output_tokens += usage.get("output_tokens", 0)
                        
                        # å°†æ¯ä¸ª chunk è¿”å›ç»™è°ƒç”¨è€…
                        yield chunk_data
                    
                    # åœ¨åŒä¸€ä¸ªä¸Šä¸‹æ–‡ä¸­å¤„ç†å·¥å…·è°ƒç”¨
                    has_tool_calls = False
                    if final_message and final_message.get("output"):
                        for output in final_message["output"]:
                            print(f"DEBUG: å¤„ç†å·¥å…·è°ƒç”¨è¾“å‡º: {output}")  # è°ƒè¯•ä¿¡æ¯
                            result = await self._process_stream_tool_call(output)
                            if result:
                                has_tool_calls = True
                                # æ·»åŠ å·¥å…·è°ƒç”¨ç»“æœåˆ°å¯¹è¯å†…å®¹
                                self.current_conversation += f"Tool <{result.tool_name}> Result Returned: {result.tool_result}\n"
                    
                print("DEBUG: æµå¼å“åº”å¤„ç†å®Œæˆ")  # è°ƒè¯•ä¿¡æ¯
                
                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œé€€å‡ºå¾ªç¯
                if not has_tool_calls:
                    print("DEBUG: æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæµå¼å¯¹è¯ç»“æŸ")
                    break
                else:
                    print("DEBUG: æœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­ä¸‹ä¸€è½®æµå¼å¯¹è¯å¤„ç†å·¥å…·ç»“æœ")
                    # ç»§ç»­whileå¾ªç¯ï¼Œè¿›è¡Œä¸‹ä¸€è½®æµå¼å¯¹è¯
                    
            except Exception as e:
                print(f"ERROR: æµå¼å¤„ç†å¼‚å¸¸: {str(e)}")  # é”™è¯¯ä¿¡æ¯
                raise
