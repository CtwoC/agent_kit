"""
èŠå¤©å¤„ç†æ¨¡å—
è´Ÿè´£AIå¯¹è¯ç”Ÿæˆã€æµå¼å†…å®¹å¤„ç†å’ŒRedisä¸­è½¬å­˜å‚¨
"""

import asyncio
import json
import uuid
from typing import Dict, Any, Optional, AsyncGenerator
from datetime import datetime

from utils.logger import get_logger
from utils.status_codes import ChatStatus, create_status_info
from models.api_models import CustomerProfile, CustomerMemory
from storage.redis_client import get_redis_client
from config.settings import get_settings

logger = get_logger(__name__)

class ChatProcessor:
    """èŠå¤©å¤„ç†æ ¸å¿ƒ"""
    
    def __init__(self):
        self.settings = get_settings()
        
    async def process(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†AIå¯¹è¯ç”Ÿæˆ
        
        Args:
            enhanced_data: åŒ…å«Profileæ•°æ®çš„å¢å¼ºè¯·æ±‚å­—å…¸
            
        Returns:
            Dict[str, Any]: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        uid = enhanced_data.get("uid")
        message = enhanced_data.get("message")
        session_id = enhanced_data.get("session_id") or str(uuid.uuid4())
        
        logger.info(f"ğŸ’¬ å¼€å§‹å¤„ç†å®¢æˆ·å¯¹è¯: {uid}")
        
        # æ›´æ–°çŠ¶æ€
        status_info = create_status_info(
            ChatStatus.PROCESSING,
            message=f"AIæ­£åœ¨ä¸ºå®¢æˆ· {uid} ç”Ÿæˆå›å¤",
            progress=0.2
        )
        
        try:
            # è·å–Rediså®¢æˆ·ç«¯ç”¨äºæµå¼å­˜å‚¨
            redis_client = await get_redis_client()
            
            # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
            context = await self._build_context(enhanced_data)
            
            # åˆå§‹åŒ–æµå¼å­˜å‚¨
            stream_key = f"stream:{uid}:{session_id}"
            await self._init_stream_storage(redis_client, stream_key, enhanced_data)
            
            # æ›´æ–°çŠ¶æ€ä¸ºæµå¼è¾“å‡º
            status_info.status = ChatStatus.STREAMING
            status_info.message = "å¼€å§‹æµå¼ç”Ÿæˆå›å¤"
            status_info.progress = 0.3
            
            # æµå¼ç”Ÿæˆå¯¹è¯
            response_content = ""
            tokens_used = 0
            
            async for chunk_data in self._generate_stream_response(context, enhanced_data):
                # å†™å…¥Redisæµå¼å­˜å‚¨
                await self._write_stream_chunk(redis_client, stream_key, chunk_data)
                
                # å¤„ç†OpenAIå®¢æˆ·ç«¯è¿”å›çš„ä¸åŒç±»å‹çš„chunk
                chunk_type = chunk_data.get("type", "")
                logger.debug(f"ğŸ“¦ å¤„ç†chunkç±»å‹: {chunk_type}")
                
                if chunk_type == "start":
                    # å¼€å§‹ä¿¡å·
                    logger.info("ğŸ¯ æ”¶åˆ°å¼€å§‹ä¿¡å·")
                elif chunk_type == "response.output_text.delta":
                    # OpenAIæµå¼å“åº”çš„æ–‡æœ¬å†…å®¹å¢é‡
                    delta_content = chunk_data.get("delta", "")
                    if delta_content:
                        response_content += delta_content
                        logger.debug(f"ğŸ“ ç´¯ç§¯å†…å®¹é•¿åº¦: {len(response_content)}")
                elif chunk_type == "response.output_text.done":
                    # OpenAIæµå¼å“åº”çš„æ–‡æœ¬å®Œæˆ
                    full_text = chunk_data.get("text", "")
                    if full_text and not response_content:
                        # å¦‚æœæ²¡æœ‰é€šè¿‡deltaç´¯ç§¯åˆ°å†…å®¹ï¼Œä½¿ç”¨å®Œæ•´æ–‡æœ¬
                        response_content = full_text
                        logger.info(f"ğŸ“„ ä½¿ç”¨å®Œæ•´æ–‡æœ¬ï¼Œé•¿åº¦: {len(response_content)}")
                elif chunk_type == "response.completed":
                    # å®Œæˆä¿¡å·ï¼ŒåŒ…å«usageä¿¡æ¯
                    logger.info("ğŸ‰ æ”¶åˆ°å®Œæˆä¿¡å·")
                elif chunk_type == "usage":
                    # Tokenä½¿ç”¨ç»Ÿè®¡
                    tokens_used = chunk_data.get("total_tokens", 0)
                    logger.info(f"ğŸ“Š Tokenä½¿ç”¨: {tokens_used}")
                elif chunk_type == "complete":
                    # æˆ‘ä»¬è‡ªå®šä¹‰çš„å®Œæˆä¿¡å·
                    logger.info("âœ… æ”¶åˆ°è‡ªå®šä¹‰å®Œæˆä¿¡å·")
                elif chunk_type == "error":
                    # é”™è¯¯ä¿¡å·
                    error_msg = chunk_data.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ æ”¶åˆ°é”™è¯¯ä¿¡å·: {error_msg}")
                    break
            
            # å®Œæˆå¤„ç†
            completion_data = {
                **enhanced_data,
                "response_content": response_content,
                "tokens_used": tokens_used,
                "session_id": session_id,
                "stream_key": stream_key,
                "processing_timestamp": datetime.now(),
                "status": create_status_info(
                    ChatStatus.COMPLETED,
                    message="AIå¯¹è¯ç”Ÿæˆå®Œæˆ",
                    progress=1.0
                )
            }
            
            # æ ‡è®°æµå¼å­˜å‚¨å®Œæˆ
            await self._complete_stream_storage(redis_client, stream_key, completion_data)
            
            logger.info(f"âœ… å®¢æˆ·å¯¹è¯å¤„ç†å®Œæˆ: {uid}, tokens: {tokens_used}")
            
            return completion_data
            
        except Exception as e:
            logger.error(f"âŒ å®¢æˆ·å¯¹è¯å¤„ç†å¤±è´¥ {uid}: {str(e)}")
            
            # åˆ›å»ºé”™è¯¯å“åº”
            error_data = {
                **enhanced_data,
                "response_content": f"å¾ˆæŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜: {str(e)}",
                "tokens_used": 0,
                "session_id": session_id,
                "processing_error": str(e),
                "processing_timestamp": datetime.now(),
                "status": create_status_info(
                    ChatStatus.ERROR,
                    message=f"å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}",
                    error_details=str(e)
                )
            }
            
            return error_data
    
    async def _build_context(self, enhanced_data: Dict[str, Any]) -> str:
        """æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        try:
            memory: CustomerMemory = enhanced_data.get("customer_memory") 
            message = enhanced_data.get("message")
            
            # ä½¿ç”¨å›ºå®šçš„é»˜è®¤ç³»ç»Ÿæç¤ºè¯
            system_prompt = self._get_default_system_prompt()
            
            # æ„å»ºå¯¹è¯å†å²ï¼ˆä»æ•°æ®åº“è·å–ï¼Œæš‚æ—¶ä½¿ç”¨ç°æœ‰é€»è¾‘ï¼‰
            conversation_history = self._build_conversation_history(memory)
            
            # ç»„åˆå®Œæ•´ä¸Šä¸‹æ–‡ï¼Œåªæ‹¼æ¥çœŸæ­£çš„å˜é‡
            context_parts = [
                f"ç³»ç»Ÿæç¤ºè¯:\n{system_prompt}",
                f"å¯¹è¯å†å²:\n{conversation_history}" if conversation_history else "",
                f"ç”¨æˆ·è¾“å…¥:\n{message}"
            ]
            
            context = "\n\n".join(part for part in context_parts if part)
            
            logger.debug(f"æ„å»ºä¸Šä¸‹æ–‡å®Œæˆï¼Œé•¿åº¦: {len(context)} å­—ç¬¦")
            return context
            
        except Exception as e:
            logger.warning(f"ä¸Šä¸‹æ–‡æ„å»ºå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ¨¡å¼: {str(e)}")
            return f"ç”¨æˆ·æ¶ˆæ¯: {enhanced_data.get('message', '')}"
    
    def _get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        # TODO: åç»­ä¼šä»æ•°æ®åº“è·å–ï¼Œç›®å‰ä½¿ç”¨å›ºå®šçš„é»˜è®¤æç¤ºè¯
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿä¸ºç”¨æˆ·æä¾›ä¼˜è´¨çš„æœåŠ¡å’Œæ”¯æŒã€‚è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œä¿æŒå‹å¥½å’Œä¸“ä¸šçš„æ€åº¦ã€‚"""
    
    def _build_conversation_history(self, memory: CustomerMemory) -> str:
        """æ„å»ºå¯¹è¯å†å²"""
        # TODO: åç»­ä¼šä»æ•°æ®åº“è·å–å†å²å¯¹è¯ï¼Œç›®å‰ä½¿ç”¨ç°æœ‰é€»è¾‘
        if not memory or not memory.short_term:
            return ""
        
        history_parts = []
        
        # æ·»åŠ é•¿æœŸè®°å¿†æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
        if memory.long_term_summary:
            history_parts.append(f"å†å²å¯¹è¯æ‘˜è¦: {memory.long_term_summary}")
        
        # æ·»åŠ æœ€è¿‘å¯¹è¯
        for conv in memory.short_term[-5:]:  # åªå–æœ€è¿‘5æ¡
            history_parts.append(f"ç”¨æˆ·: {conv.customer_message}")
            history_parts.append(f"AI: {conv.agent_message}")
        
        return "\n".join(history_parts)
    
    async def _init_stream_storage(self, redis_client, stream_key: str, enhanced_data: Dict[str, Any]):
        """åˆå§‹åŒ–æµå¼å­˜å‚¨"""
        try:
            init_data = {
                "uid": enhanced_data.get("uid"),
                "session_id": enhanced_data.get("session_id"),
                "start_time": datetime.now().isoformat(),
                "status": "streaming",
                "chunks": []
            }
            
            await redis_client.hset(stream_key, "metadata", json.dumps(init_data))
            await redis_client.expire(stream_key, self.settings.redis.stream_ttl)
            
        except Exception as e:
            logger.warning(f"æµå¼å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    async def _write_stream_chunk(self, redis_client, stream_key: str, chunk_data: Dict[str, Any]):
        """å†™å…¥æµå¼æ•°æ®å—"""
        try:
            chunk_with_timestamp = {
                **chunk_data,
                "timestamp": datetime.now().isoformat()
            }
            
            await redis_client.lpush(
                f"{stream_key}:chunks", 
                json.dumps(chunk_with_timestamp, ensure_ascii=False)
            )
            
            # é™åˆ¶chunksæ•°é‡ï¼Œé¿å…å†…å­˜è¿‡åº¦ä½¿ç”¨
            await redis_client.ltrim(f"{stream_key}:chunks", 0, 1000)
            
        except Exception as e:
            logger.warning(f"æµå¼æ•°æ®å†™å…¥å¤±è´¥: {str(e)}")
    
    async def _complete_stream_storage(self, redis_client, stream_key: str, completion_data: Dict[str, Any]):
        """å®Œæˆæµå¼å­˜å‚¨"""
        try:
            completion_info = {
                "status": "completed",
                "end_time": datetime.now().isoformat(),
                "total_tokens": completion_data.get("tokens_used", 0),
                "response_length": len(completion_data.get("response_content", ""))
            }
            
            await redis_client.hset(stream_key, "completion", json.dumps(completion_info))
            
        except Exception as e:
            logger.warning(f"æµå¼å­˜å‚¨å®Œæˆæ ‡è®°å¤±è´¥: {str(e)}")
    
    async def _generate_stream_response(self, context: str, enhanced_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """ç”Ÿæˆæµå¼å“åº”ï¼ˆè°ƒç”¨OpenAIå®¢æˆ·ç«¯ï¼‰"""
        client = None
        try:
            # ä½¿ç”¨æœåŠ¡é…ç½®ï¼ˆä¸å…è®¸è¯·æ±‚è¦†ç›–ï¼‰
            api_key = self.settings.openai.api_key
            base_url = self.settings.openai.base_url
            # mcp_url = "http://39.103.228.66:8165/mcp/"
            mcp_urls = []  # MCPæœåŠ¡åœ°å€ï¼Œå¯ä»¥è€ƒè™‘åŠ å…¥é…ç½®
            
            logger.info(f"ğŸ”§ å‡†å¤‡åˆ›å»ºOpenAIå®¢æˆ·ç«¯...")
            logger.info(f"  - æ¨¡å‹: {self.settings.openai.model}")
            logger.info(f"  - Base URL: {base_url}")
            logger.info(f"  - MCP URL: {mcp_urls}")
            logger.info(f"  - API Keyå‰8ä½: {api_key[:8] if api_key else 'None'}")
            
            # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
            from client.openai_client import OpenAIClient
            
            client_kwargs = {}
            if base_url:
                client_kwargs["base_url"] = base_url
            
            # åˆ›å»ºå¹¶åˆå§‹åŒ–å®¢æˆ·ç«¯
            logger.info("ğŸš€ åˆ›å»ºOpenAIå®¢æˆ·ç«¯å®ä¾‹...")
            client = OpenAIClient(
                api_key=api_key,
                model=self.settings.openai.model,
                mcp_urls=mcp_urls,
                **client_kwargs
            )
            
            # æ‰‹åŠ¨åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆè°ƒç”¨__aenter__ï¼‰
            logger.info("ğŸ”Œ åˆå§‹åŒ–å®¢æˆ·ç«¯è¿æ¥...")
            await client.__aenter__()
            logger.info("âœ… å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
            
            # å‘é€å¼€å§‹ä¿¡å·
            logger.info("ğŸ¯ å¼€å§‹æµå¼å¯¹è¯ç”Ÿæˆ...")
            yield {"type": "start", "message": "å¼€å§‹ç”Ÿæˆå›å¤"}
            
            # è°ƒç”¨æµå¼å¯¹è¯
            chunk_count = 0
            logger.info("ğŸ“¡ è°ƒç”¨client.stream_chat...")
            async for chunk in client.stream_chat(context):
                chunk_count += 1
                logger.debug(f"ğŸ“¦ æ”¶åˆ°ç¬¬{chunk_count}ä¸ªchunk: {chunk.get('type', 'unknown')}")
                # ç›´æ¥yield OpenAIå®¢æˆ·ç«¯è¿”å›çš„chunk
                yield chunk
            
            logger.info(f"âœ¨ æµå¼å¯¹è¯å®Œæˆï¼Œå…±æ”¶åˆ°{chunk_count}ä¸ªchunks")
            
            # å‘é€usageç»Ÿè®¡ä¿¡æ¯ï¼ˆä»å®¢æˆ·ç«¯è·å–ï¼‰
            if hasattr(client, 'usage') and client.usage:
                logger.info(f"ğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡: è¾“å…¥={client.usage.input_tokens}, è¾“å‡º={client.usage.output_tokens}")
                yield {
                    "type": "usage",
                    "input_tokens": client.usage.input_tokens,
                    "output_tokens": client.usage.output_tokens,
                    "total_tokens": client.usage.input_tokens + client.usage.output_tokens
                }
            
            # å‘é€å®Œæˆä¿¡å·
            logger.info("ğŸ‰ å‘é€å®Œæˆä¿¡å·")
            yield {"type": "complete", "message": "å›å¤ç”Ÿæˆå®Œæˆ"}
            
        except Exception as e:
            logger.error(f"âŒ æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"   é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            yield {"type": "error", "error": str(e)}
            
        finally:
            # ç¡®ä¿å®¢æˆ·ç«¯è¢«æ­£ç¡®å…³é—­
            if client:
                try:
                    logger.info("ğŸ”Œ å…³é—­å®¢æˆ·ç«¯è¿æ¥...")
                    await client.__aexit__(None, None, None)
                    logger.info("âœ… å®¢æˆ·ç«¯å…³é—­å®Œæˆ")
                except Exception as e:
                    logger.warning(f"âš ï¸ å®¢æˆ·ç«¯å…³é—­å¤±è´¥: {str(e)}") 