#!/usr/bin/env python3
"""
åŸºç¡€Agentç¤ºä¾‹ - æœ€ç®€å•çš„FastAPIå¼‚æ­¥AgentæœåŠ¡

åŠŸèƒ½ï¼š
- æ¥æ”¶POSTè¯·æ±‚çš„ç”¨æˆ·æç¤ºè¯
- ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯
- è°ƒç”¨OpenAI GPTæ¨¡å‹
- è¿”å›å“åº”
- æ²¡æœ‰è®°å¿†ã€CoTç­‰å…¶ä»–åŠŸèƒ½
"""

import sys
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from contextlib import asynccontextmanager
import uvicorn

# æ·»åŠ clientç›®å½•åˆ°Pythonè·¯å¾„
# å½“å‰æ–‡ä»¶ä½äº: agent/examples/basic_agent/main.py
# éœ€è¦å›åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç„¶åè¿›å…¥clientç›®å½•
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
client_path = os.path.join(project_root, 'client')
sys.path.insert(0, client_path)

from openai_client import OpenAIClient

# å…¨å±€å˜é‡å­˜å‚¨å®¢æˆ·ç«¯
openai_client = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global openai_client
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    print("ğŸš€ åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯...")
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
    
    # MCPæœåŠ¡å™¨é…ç½®ï¼ˆå¯é€‰ï¼‰
    mcp_url = os.getenv("MCP_URL", "http://39.103.228.66:8165/mcp/")
    base_url = os.getenv("OPENAI_BASE_URL", "http://43.130.31.174:8003/v1")
    
    openai_client = OpenAIClient(
        api_key=api_key,
        base_url=base_url,
        mcp_urls=[mcp_url] if mcp_url else None
    )
    
    # è¿›å…¥ä¸Šä¸‹æ–‡
    await openai_client.__aenter__()
    print("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†èµ„æº
    print("ğŸ”„ æ­£åœ¨å…³é—­OpenAIå®¢æˆ·ç«¯...")
    await openai_client.__aexit__(None, None, None)
    print("âœ… OpenAIå®¢æˆ·ç«¯å·²å…³é—­")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="åŸºç¡€AgentæœåŠ¡",
    description="æœ€ç®€å•çš„Agentç¤ºä¾‹ - ä½¿ç”¨OpenAI GPTæ¨¡å‹å›ç­”ç”¨æˆ·é—®é¢˜",
    version="1.0.0",
    lifespan=lifespan
)

# è¯·æ±‚æ¨¡å‹
class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    message: str
    system_prompt: str = None  # å¯é€‰çš„è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯

# å“åº”æ¨¡å‹  
class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    response: str
    usage: dict
    model: str

# é»˜è®¤ç³»ç»Ÿæç¤ºè¯
DEFAULT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨ã€è¯šå®å’Œæ— å®³çš„AIåŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ç†è§£ç”¨æˆ·çš„é—®é¢˜æˆ–éœ€æ±‚
2. æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”
3. ä¿æŒå‹å¥½å’Œä¸“ä¸šçš„è¯­æ°”
4. å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨å…¶ä»–è¯­è¨€ã€‚"""

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - æœåŠ¡çŠ¶æ€æ£€æŸ¥"""
    return {
        "message": "ğŸ¤– åŸºç¡€AgentæœåŠ¡è¿è¡Œä¸­",
        "status": "healthy",
        "version": "1.0.0"
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "client_initialized": openai_client is not None
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    èŠå¤©ç«¯ç‚¹ - å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›AIå“åº”
    
    Args:
        request: åŒ…å«ç”¨æˆ·æ¶ˆæ¯å’Œå¯é€‰ç³»ç»Ÿæç¤ºè¯çš„è¯·æ±‚
        
    Returns:
        åŒ…å«AIå“åº”ã€ä½¿ç”¨ç»Ÿè®¡å’Œæ¨¡å‹ä¿¡æ¯çš„å“åº”
    """
    try:
        if not openai_client:
            raise HTTPException(status_code=500, detail="OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        # æ„å»ºå®Œæ•´çš„æç¤ºè¯
        system_prompt = request.system_prompt or DEFAULT_SYSTEM_PROMPT
        full_prompt = f"ç³»ç»Ÿæç¤ºè¯ï¼š\n{system_prompt}\n\nç”¨æˆ·æ¶ˆæ¯ï¼š\n{request.message}"
        
        print(f"ğŸ“ æ”¶åˆ°ç”¨æˆ·æ¶ˆæ¯: {request.message[:50]}{'...' if len(request.message) > 50 else ''}")
        
        # è°ƒç”¨OpenAIå®¢æˆ·ç«¯
        response = await openai_client.chat(full_prompt)
        
        # æå–å“åº”å†…å®¹
        assistant_message = response["choices"][0]["message"]["content"]
        
        # æ„å»ºå“åº”
        chat_response = ChatResponse(
            response=assistant_message,
            usage={
                "input_tokens": openai_client.usage.input_tokens,
                "output_tokens": openai_client.usage.output_tokens,
                "total_tokens": openai_client.usage.total_tokens,
                "total_cost": round(openai_client.usage.total_cost, 6)
            },
            model=response.get("model", "unknown")
        )
        
        print(f"âœ… å“åº”ç”ŸæˆæˆåŠŸï¼Œæ€»è®¡ {openai_client.usage.total_tokens} tokens")
        
        return chat_response
        
    except Exception as e:
        print(f"âŒ å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

@app.get("/stats")
async def get_stats():
    """è·å–ä½¿ç”¨ç»Ÿè®¡"""
    if not openai_client:
        raise HTTPException(status_code=500, detail="OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
    return {
        "usage_stats": {
            "input_tokens": openai_client.usage.input_tokens,
            "output_tokens": openai_client.usage.output_tokens, 
            "total_tokens": openai_client.usage.total_tokens,
            "input_cost": round(openai_client.usage.input_cost, 6),
            "output_cost": round(openai_client.usage.output_cost, 6),
            "total_cost": round(openai_client.usage.total_cost, 6)
        },
        "available_tools": len(openai_client.get_available_tools()) if openai_client else 0
    }

@app.post("/reset-stats")
async def reset_stats():
    """é‡ç½®ä½¿ç”¨ç»Ÿè®¡"""
    if not openai_client:
        raise HTTPException(status_code=500, detail="OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
    openai_client.usage.reset()
    return {"message": "ä½¿ç”¨ç»Ÿè®¡å·²é‡ç½®"}

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨åŸºç¡€AgentæœåŠ¡...")
    print("ğŸ“– è¯·ç¡®ä¿è®¾ç½®äº†ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
    print("   - OPENAI_API_KEY: OpenAI APIå¯†é’¥")
    print("   - MCP_URL: MCPæœåŠ¡å™¨URL (å¯é€‰)")
    print("   - OPENAI_BASE_URL: OpenAI APIåŸºç¡€URL (å¯é€‰)")
    print()
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8080,
        reload=True,
        log_level="info"
    ) 