#!/usr/bin/env python3
"""
èŠå¤©å¤„ç†æ¨¡å— - è´Ÿè´£èŠå¤©é€»è¾‘å¤„ç†
åŒ…æ‹¬promptç»„åˆã€OpenAIå®¢æˆ·ç«¯åˆ›å»ºã€æµå¼å“åº”å¤„ç†
"""

import sys
import os
import json
import asyncio
from typing import Optional, AsyncGenerator

# æ·»åŠ clientç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
client_path = os.path.join(project_root, 'client')
sys.path.insert(0, client_path)

from openai_client import OpenAIClient

# é»˜è®¤ç³»ç»Ÿæç¤ºè¯
DEFAULT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’ŒååŠ©å®Œæˆä»»åŠ¡ã€‚ä½ æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

1. å‹å–„ä¸”ä¸“ä¸šçš„äº¤æµæ–¹å¼
2. èƒ½å¤Ÿä½¿ç”¨å¯ç”¨çš„å·¥å…·æ¥è¾…åŠ©å›ç­”é—®é¢˜
3. æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯
4. æ ¹æ®ä¸Šä¸‹æ–‡ç»™å‡ºæ°å½“çš„å›åº”

è¯·ç”¨ä¸­æ–‡ä¸ç”¨æˆ·äº¤æµï¼Œä¿æŒç®€æ´æ˜äº†çš„å›ç­”é£æ ¼ã€‚"""

class ChatProcessor:
    """èŠå¤©å¤„ç†å™¨"""
    
    @staticmethod
    def create_client() -> OpenAIClient:
        """
        åˆ›å»ºæ–°çš„OpenAIå®¢æˆ·ç«¯
        
        Returns:
            OpenAIClient: é…ç½®å¥½çš„OpenAIå®¢æˆ·ç«¯å®ä¾‹
            
        Raises:
            ValueError: å¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®
        """
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
        
        mcp_url = os.getenv("MCP_URL", "http://39.103.228.66:8165/mcp/")
        base_url = os.getenv("OPENAI_BASE_URL", "http://43.130.31.174:8003/v1")
        
        return OpenAIClient(
            api_key=api_key,
            base_url=base_url,
            mcp_urls=[mcp_url] if mcp_url else None
        )
    
    @staticmethod
    def build_prompt(message: str, system_prompt: Optional[str] = None) -> str:
        """
        æ„å»ºå®Œæ•´çš„prompt
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: å¯é€‰çš„è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            str: ç»„åˆåçš„å®Œæ•´prompt
        """
        system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
        return f"ç³»ç»Ÿæç¤ºè¯ï¼š\n{system_prompt}\n\nç”¨æˆ·æ¶ˆæ¯ï¼š\n{message}"
    
    @staticmethod
    async def process_stream_chat(
        message: str, 
        uid: str, 
        system_prompt: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        å¤„ç†æµå¼èŠå¤©è¯·æ±‚
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            uid: ç”¨æˆ·ID
            system_prompt: å¯é€‰çš„è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
            
        Yields:
            str: æµå¼å“åº”æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
        """
        client = None
        try:
            print(f"ğŸ“ ç”¨æˆ· {uid} å¼€å§‹æµå¼å¤„ç†: {message[:50]}{'...' if len(message) > 50 else ''}")
            
            # åˆ›å»ºå¹¶åˆå§‹åŒ–å®¢æˆ·ç«¯
            client = ChatProcessor.create_client()
            await client.__aenter__()
            
            # æ„å»ºprompt
            full_prompt = ChatProcessor.build_prompt(message, system_prompt)
            
            # å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'message': 'å¼€å§‹ç”Ÿæˆå“åº”...', 'uid': uid}, ensure_ascii=False)}\n\n"
            
            # å¤„ç†æµå¼å“åº”
            content_buffer = ""
            async for chunk in client.stream_chat(full_prompt):
                chunk_type = chunk.get("type", "")
                
                # å¤„ç†æ–‡æœ¬å†…å®¹çš„å¢é‡æ›´æ–°
                if chunk_type == "response.output_text.delta":
                    delta_text = chunk.get("delta", "")
                    if delta_text:
                        content_buffer += delta_text
                        yield f"data: {json.dumps({'type': 'content', 'chunk': delta_text, 'uid': uid}, ensure_ascii=False)}\n\n"
                        await asyncio.sleep(0.01)  # æµå¼æ•ˆæœå»¶è¿Ÿ
                
                # å¤„ç†å·¥å…·è°ƒç”¨å¼€å§‹
                elif chunk_type == "response.output_item.added":
                    item = chunk.get("item", {})
                    if item.get("type") == "function_call":
                        function_name = item.get("name", "unknown")
                        print(f"ğŸ”§ ç”¨æˆ· {uid} è°ƒç”¨å·¥å…·: {function_name}")
                        yield f"data: {json.dumps({'type': 'tool_call', 'tool': function_name, 'status': 'started', 'uid': uid}, ensure_ascii=False)}\n\n"
                
                # å¤„ç†å·¥å…·è°ƒç”¨å®Œæˆ
                elif chunk_type == "response.output_item.done":
                    item = chunk.get("item", {})
                    if item.get("type") == "function_call" and item.get("status") == "completed":
                        function_name = item.get("name", "unknown")
                        arguments = item.get("arguments", "{}")
                        print(f"âœ… ç”¨æˆ· {uid} å·¥å…·å®Œæˆ: {function_name}")
                        yield f"data: {json.dumps({'type': 'tool_call', 'tool': function_name, 'status': 'completed', 'arguments': arguments, 'uid': uid}, ensure_ascii=False)}\n\n"
                
                # å¤„ç†å“åº”åˆ›å»º
                elif chunk_type == "response.created":
                    yield f"data: {json.dumps({'type': 'thinking', 'message': 'æ­£åœ¨æ€è€ƒ...', 'uid': uid}, ensure_ascii=False)}\n\n"
                
                # å¤„ç†å†…å®¹éƒ¨åˆ†æ·»åŠ 
                elif chunk_type == "response.content_part.added":
                    yield f"data: {json.dumps({'type': 'content_start', 'message': 'å¼€å§‹ç”Ÿæˆå†…å®¹...', 'uid': uid}, ensure_ascii=False)}\n\n"
            
            # å‘é€å®Œæˆäº‹ä»¶å’Œç»Ÿè®¡ä¿¡æ¯
            completion_data = {
                "type": "complete",
                "full_content": content_buffer,
                "uid": uid,
                "usage": {
                    "input_tokens": client.usage.input_tokens,
                    "output_tokens": client.usage.output_tokens,
                    "total_tokens": client.usage.total_tokens,
                    "total_cost": round(client.usage.total_cost, 6)
                }
            }
            yield f"data: {json.dumps(completion_data, ensure_ascii=False)}\n\n"
            
            print(f"âœ… ç”¨æˆ· {uid} æµå¼å“åº”å®Œæˆï¼Œæ€»è®¡ {client.usage.total_tokens} tokens")
            
        except Exception as e:
            print(f"âŒ ç”¨æˆ· {uid} å¤„ç†æµå¼è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            error_data = {
                "type": "error",
                "error": f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "uid": uid
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        
        finally:
            # ç¡®ä¿å…³é—­å®¢æˆ·ç«¯
            if client:
                try:
                    await client.__aexit__(None, None, None)
                except Exception as e:
                    print(f"âš ï¸ å…³é—­å®¢æˆ·ç«¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    @staticmethod
    async def process_chat(
        message: str, 
        uid: str, 
        system_prompt: Optional[str] = None
    ) -> dict:
        """
        å¤„ç†éæµå¼èŠå¤©è¯·æ±‚
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            uid: ç”¨æˆ·ID
            system_prompt: å¯é€‰çš„è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            dict: åŒ…å«å“åº”å†…å®¹å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
            
        Raises:
            Exception: å¤„ç†è¿‡ç¨‹ä¸­çš„å„ç§å¼‚å¸¸
        """
        client = None
        try:
            print(f"ğŸ“ ç”¨æˆ· {uid} å¼€å§‹éæµå¼å¤„ç†: {message[:50]}{'...' if len(message) > 50 else ''}")
            
            # åˆ›å»ºå¹¶åˆå§‹åŒ–å®¢æˆ·ç«¯
            client = ChatProcessor.create_client()
            await client.__aenter__()
            
            # æ„å»ºprompt
            full_prompt = ChatProcessor.build_prompt(message, system_prompt)
            
            # è°ƒç”¨OpenAIå®¢æˆ·ç«¯
            response = await client.chat(full_prompt)
            
            # æå–å“åº”å†…å®¹
            assistant_message = response["choices"][0]["message"]["content"]
            
            # æ„å»ºå“åº”
            chat_response = {
                "response": assistant_message,
                "uid": uid,
                "usage": {
                    "input_tokens": client.usage.input_tokens,
                    "output_tokens": client.usage.output_tokens,
                    "total_tokens": client.usage.total_tokens,
                    "total_cost": round(client.usage.total_cost, 6)
                },
                "model": response.get("model", "unknown"),
                "type": "non_stream"
            }
            
            print(f"âœ… ç”¨æˆ· {uid} éæµå¼å“åº”å®Œæˆï¼Œæ€»è®¡ {client.usage.total_tokens} tokens")
            
            return chat_response
            
        finally:
            # ç¡®ä¿å…³é—­å®¢æˆ·ç«¯
            if client:
                try:
                    await client.__aexit__(None, None, None)
                except Exception as e:
                    print(f"âš ï¸ å…³é—­å®¢æˆ·ç«¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}") 